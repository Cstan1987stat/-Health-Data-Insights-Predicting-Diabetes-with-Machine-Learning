{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcEWlm29bO5y"
      },
      "source": [
        "*Adjusting the Preprocessing Strategy*\n",
        "\n",
        "Before diving into the following data, I believe it would be helpful to refine the preprocessing strategy.\n",
        "So far, I have been separating my predictor variables into either continuous or discrete categories. However, there may be differences within discrete variables that need to be addressed. The reason for this consideration is that I have been one-hot encoding all discrete variables, which is generally fine. However, if I start adding interaction terms, this approach could significantly increase the dimensionality of the data. Since we already have a large number of instances in the training set, a substantial increase in dimensionality would greatly increase the computational time required for each step.\n",
        "\n",
        "As mentioned in the initial notebook, when our data is in numeric format, values like 2 are interpreted as greater than 1. However, in some cases, there is no mathematical relationship or inherent order between such values. To handle this, we can classify discrete variables as either nominal or ordinal:\n",
        "\n",
        "Nominal variables are discrete variables with no inherent order between categories.\n",
        "Ordinal variables have an inherent order or ranking between categories.\n",
        "Below are examples of each:\n",
        "\n",
        "**Ordinal Variables**\n",
        "* general_health: As this value increases, an individual's health worsens, indicating an inherent order.\n",
        "* Other examples: physical_activity_150, education_level, income_group, smoking_status, physical_health_days, and mental_health_days.\n",
        "\n",
        "**Nominal Variables**\n",
        "* sex: This variable represents either male or female. There is no inherent order, making it nominal.\n",
        "* Other examples: has_health_plan, meets_aerobic_guidelines, muscle_strengthening, high_blood_pressure, high_cholesterol, heart_disease, lifetime_asthma, arthritis, alcohol_consumption, binge_drinking, heavy_drinking, and difficulty_walking.\n",
        "\n",
        "Since ordinal variables have an inherent order and are already represented by numerical values, there is no need to one-hot encode them. However, nominal variables should be one-hot encoded to ensure they are appropriately represented in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUehhMSZbO51",
        "outputId": "67d087e8-eeaa-4cc7-d1ba-fd82442e3b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 217981 entries, 0 to 217980\n",
            "Data columns (total 24 columns):\n",
            " #   Column                    Non-Null Count   Dtype  \n",
            "---  ------                    --------------   -----  \n",
            " 0   general_health            217981 non-null  float64\n",
            " 1   physical_health_days      217981 non-null  float64\n",
            " 2   mental_health_days        217981 non-null  float64\n",
            " 3   has_health_plan           217981 non-null  float64\n",
            " 4   meets_aerobic_guidelines  217981 non-null  float64\n",
            " 5   physical_activity_150min  217981 non-null  float64\n",
            " 6   muscle_strengthening      217981 non-null  float64\n",
            " 7   high_blood_pressure       217981 non-null  float64\n",
            " 8   high_cholesterol          217981 non-null  float64\n",
            " 9   heart_disease             217981 non-null  float64\n",
            " 10  lifetime_asthma           217981 non-null  float64\n",
            " 11  arthritis                 217981 non-null  float64\n",
            " 12  sex                       217981 non-null  float64\n",
            " 13  age                       217981 non-null  float64\n",
            " 14  height_inches             217981 non-null  float64\n",
            " 15  bmi                       217981 non-null  float64\n",
            " 16  education_level           217981 non-null  float64\n",
            " 17  income_group              217981 non-null  float64\n",
            " 18  smoking_status            217981 non-null  float64\n",
            " 19  alcohol_consumption       217981 non-null  float64\n",
            " 20  binge_drinking            217981 non-null  float64\n",
            " 21  heavy_drinking            217981 non-null  float64\n",
            " 22  diabetes_status           217981 non-null  float64\n",
            " 23  difficulty_walking        217981 non-null  float64\n",
            "dtypes: float64(24)\n",
            "memory usage: 39.9 MB\n"
          ]
        }
      ],
      "source": [
        "# Importing pandas and numpy library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loading in data frame\n",
        "df = pd.read_csv('df.csv')\n",
        "\n",
        "# Deleting unnecessary column and symbolizing all non-diabetic records with 0 instead of 3.\n",
        "del df['Unnamed: 0']\n",
        "df['diabetes_status'] = df['diabetes_status'].replace(3,0)\n",
        "\n",
        "# Viewing the non-null count and data type for each feature.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the data type for each feature to float32 from float64 to reduce memory needed to store data.\n",
        "for i in df.columns:\n",
        "  df[i] = df[i].astype('float32')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rx656h3cBEd",
        "outputId": "1fce07df-a464-4f3b-ff33-75923869f704"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 217981 entries, 0 to 217980\n",
            "Data columns (total 24 columns):\n",
            " #   Column                    Non-Null Count   Dtype  \n",
            "---  ------                    --------------   -----  \n",
            " 0   general_health            217981 non-null  float32\n",
            " 1   physical_health_days      217981 non-null  float32\n",
            " 2   mental_health_days        217981 non-null  float32\n",
            " 3   has_health_plan           217981 non-null  float32\n",
            " 4   meets_aerobic_guidelines  217981 non-null  float32\n",
            " 5   physical_activity_150min  217981 non-null  float32\n",
            " 6   muscle_strengthening      217981 non-null  float32\n",
            " 7   high_blood_pressure       217981 non-null  float32\n",
            " 8   high_cholesterol          217981 non-null  float32\n",
            " 9   heart_disease             217981 non-null  float32\n",
            " 10  lifetime_asthma           217981 non-null  float32\n",
            " 11  arthritis                 217981 non-null  float32\n",
            " 12  sex                       217981 non-null  float32\n",
            " 13  age                       217981 non-null  float32\n",
            " 14  height_inches             217981 non-null  float32\n",
            " 15  bmi                       217981 non-null  float32\n",
            " 16  education_level           217981 non-null  float32\n",
            " 17  income_group              217981 non-null  float32\n",
            " 18  smoking_status            217981 non-null  float32\n",
            " 19  alcohol_consumption       217981 non-null  float32\n",
            " 20  binge_drinking            217981 non-null  float32\n",
            " 21  heavy_drinking            217981 non-null  float32\n",
            " 22  diabetes_status           217981 non-null  float32\n",
            " 23  difficulty_walking        217981 non-null  float32\n",
            "dtypes: float32(24)\n",
            "memory usage: 20.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolating target variable.\n",
        "df_target = df['diabetes_status']\n",
        "del df['diabetes_status']\n",
        "\n",
        "# Importing functions to split up and transform our data.\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Creating lists to contain the continuous, ordinal, and nominal variable names\n",
        "continuous = ['age','height_inches','bmi']\n",
        "\n",
        "ordinal = ['general_health', 'physical_health_days', 'mental_health_days', 'physical_activity_150min',\n",
        "           'education_level', 'income_group', 'smoking_status']\n",
        "\n",
        "nominal = ['has_health_plan', 'meets_aerobic_guidelines', 'muscle_strengthening',\n",
        "           'high_blood_pressure', 'high_cholesterol', 'heart_disease',\n",
        "           'lifetime_asthma', 'arthritis', 'sex',\n",
        "           'alcohol_consumption', 'binge_drinking', 'heavy_drinking',\n",
        "           'difficulty_walking']\n",
        "\n",
        "# Splitting up predictor and target variable into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.30, random_state=22, stratify=df_target)\n",
        "# Utilizing stratify parameters helps ensure that the percentage or diabetic and non-diabetic individuals are around the same in the training and tests sets.\n",
        "\n",
        "# Creating pipeline to logarithmically transform and scale all continuous variables.\n",
        "continuous_pipeline = Pipeline([\n",
        "    ('log', FunctionTransformer(func=np.log1p)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "# Creating pipeline to scale each ordinal variable.\n",
        "ordinal_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "# Creating pipeline to one-hot encode all nominal variables while dropping the first and then scale each variable.\n",
        "nominal_pipeline = Pipeline([\n",
        "    ('one_hot', OneHotEncoder(sparse_output=False, drop='first')),\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "# Creating a column transform to send all continuous to the continuous_pipeline, all ordinal variables to the ordinal_pipeline, and all nominal variables to the nominal_pipeline.\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('cont', continuous_pipeline, continuous),\n",
        "    ('ord', ordinal_pipeline, ordinal),\n",
        "    ('nom', nominal_pipeline, nominal),\n",
        "])\n",
        "\n",
        "\n",
        "# Fitting column transform with training data and then transforming training data using the fitted column transformer.\n",
        "X_train1 = column_transformer.fit_transform(X_train)\n",
        "# Transforming testing data using the fitted column transform.\n",
        "X_test1 = column_transformer.transform(X_test)\n",
        "\n",
        "# Creating list for column names.\n",
        "nominal_columns = column_transformer.named_transformers_['nom']['one_hot'].get_feature_names_out(nominal).tolist()\n",
        "correct_columns = continuous + ordinal + nominal_columns\n",
        "\n",
        "# Creating data frames based on X_train1 and X_test1 for feature importance analysis later one.\n",
        "X_train1 = pd.DataFrame(X_train1, columns=correct_columns)\n",
        "X_test1 = pd.DataFrame(X_test1, columns=correct_columns)"
      ],
      "metadata": {
        "id": "InALV0qUbz4L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBJNoOYs1lyY",
        "outputId": "f59ba826-4153-49d7-8101-b1be2c009567"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'height_inches', 'bmi', 'general_health', 'physical_health_days',\n",
              "       'mental_health_days', 'physical_activity_150min', 'education_level',\n",
              "       'income_group', 'smoking_status', 'has_health_plan_1.0',\n",
              "       'meets_aerobic_guidelines_1.0', 'muscle_strengthening_1.0',\n",
              "       'high_blood_pressure_1.0', 'high_cholesterol_1.0', 'heart_disease_1.0',\n",
              "       'lifetime_asthma_1.0', 'arthritis_1.0', 'sex_1.0',\n",
              "       'alcohol_consumption_1.0', 'binge_drinking_1.0', 'heavy_drinking_1.0',\n",
              "       'difficulty_walking_1.0'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1 = X_train1.rename(columns={'has_health_plan_1.0': 'has_health_plan',\n",
        "                                    'meets_aerobic_guidelines_1.0': 'meets_aerobic_guidelines',\n",
        "                                    'muscle_strengthening_1.0': 'meets_muscle_strengthening',\n",
        "                                    'high_blood_pressure_1.0': 'no_high_blood_pressure',\n",
        "                                    'high_cholesterol_1.0': 'no_high_cholesterol',\n",
        "                                    'heart_disease_1.0': 'yes_heart_disease',\n",
        "                                    'lifetime_asthma_1.0': 'no_asthma',\n",
        "                                    'arthritis_1.0': 'no_arthritis',\n",
        "                                    'sex_1.0': 'male_sex',\n",
        "                                    'alcohol_consumption_1.0': 'yes_alcohol_consumption',\n",
        "                                    'binge_drinking_1.0': 'no_binge_drinking',\n",
        "                                    'heavy_drinking_1.0': 'no_heavy_drinking',\n",
        "                                    'difficulty_walking_1.0': 'yes_difficulty_walking'})\n",
        "\n",
        "X_test1 = X_test1.rename(columns={'has_health_plan_1.0': 'has_health_plan',\n",
        "                                    'meets_aerobic_guidelines_1.0': 'meets_aerobic_guidelines',\n",
        "                                    'muscle_strengthening_1.0': 'meets_muscle_strengthening',\n",
        "                                    'high_blood_pressure_1.0': 'no_high_blood_pressure',\n",
        "                                    'high_cholesterol_1.0': 'no_high_cholesterol',\n",
        "                                    'heart_disease_1.0': 'yes_heart_disease',\n",
        "                                    'lifetime_asthma_1.0': 'no_asthma',\n",
        "                                    'arthritis_1.0': 'no_arthritis',\n",
        "                                    'sex_1.0': 'male_sex',\n",
        "                                    'alcohol_consumption_1.0': 'yes_alcohol_consumption',\n",
        "                                    'binge_drinking_1.0': 'no_binge_drinking',\n",
        "                                    'heavy_drinking_1.0': 'no_heavy_drinking',\n",
        "                                    'difficulty_walking_1.0': 'yes_difficulty_walking'})\n",
        "\n"
      ],
      "metadata": {
        "id": "E-NBe6sw17Kv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TjTYl6oIbO51"
      },
      "outputs": [],
      "source": [
        "# Importing LogisticRegression and f1_score functions.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "# Importing randomized search cv function, stratified fold function for cv.\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuQubi3ebO52",
        "outputId": "ca052092-c0c1-4899-a518-bcbac6aed4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_score on training data: 0.46\n",
            "f1_score on testing data: 0.46\n"
          ]
        }
      ],
      "source": [
        "# Creating a LogisticRegression model, lg, with random state set to 42 and class_weight to balanced.\n",
        "lg = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Fitting lg with training data.\n",
        "lg.fit(X_train1, y_train)\n",
        "\n",
        "# Calculating f1_score on training data.\n",
        "pred_target = lg.predict(X_train1)\n",
        "f1_train = f1_score(y_train, pred_target, pos_label=1.0)\n",
        "print('f1_score on training data:', np.round(f1_train,2))\n",
        "\n",
        "# Calculating f1_score on testing data.\n",
        "pred_target = lg.predict(X_test1)\n",
        "f1_test = f1_score(y_test, pred_target,pos_label=1.0)\n",
        "print('f1_score on testing data:', np.round(f1_test,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXImUJH6bO52",
        "outputId": "df769a06-1c4c-43c7-e3ef-5c47334108ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
            "{'max_iter': 250, 'class_weight': {0: 1, 1: 3.5}}\n",
            "f1_score on training data: 0.47\n",
            "f1_score on testing data: 0.48\n"
          ]
        }
      ],
      "source": [
        "# Creating list to house dictionary of possible parameter values.\n",
        "param_dist = [{\n",
        "    'max_iter': [25, 50, 100, 150, 250],\n",
        "    'class_weight': [{0:1, 1:w} for w in [1, 2, 3, 3.5, 4, 5.69]]\n",
        "}]\n",
        "\n",
        "# Creating initial LogisticRegression model.\n",
        "lg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Creating initial RandomizedSearchCV function.\n",
        "rand_search = RandomizedSearchCV(lg, param_distributions=param_dist,\n",
        "                                scoring= make_scorer(f1_score, pos_label=1.0),\n",
        "                                cv=StratifiedKFold(n_splits=10),\n",
        "                                verbose=1, n_iter=20, random_state=22)\n",
        "# Fitting rand_search with training data.\n",
        "rand_search.fit(X_train1, y_train)\n",
        "\n",
        "# Printing the best parameters from the best estimator from rand_search.\n",
        "print(rand_search.best_params_)\n",
        "\n",
        "best_estimator = rand_search.best_estimator_\n",
        "# Calculating f1_score on training data.\n",
        "pred_target = best_estimator.predict(X_train1)\n",
        "f1_train = f1_score(y_train, pred_target, pos_label=1.0)\n",
        "print('f1_score on training data:', np.round(f1_train,2))\n",
        "\n",
        "# Calculating f1_score on testing data.\n",
        "pred_target = best_estimator.predict(X_test1)\n",
        "f1_test = f1_score(y_test, pred_target,pos_label=1.0)\n",
        "print('f1_score on testing data:', np.round(f1_test,2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_iter = 250, class_weight {0: 1, 1: 3.5}"
      ],
      "metadata": {
        "id": "AjV4XB2KO80D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in confusion_matrix and scoring metric functions from scikit-learn\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Fitting LogisticRegression model with parameters determined from randomizedsearchcv.\n",
        "lg = LogisticRegression(random_state=42, class_weight={0:1, 1:3.5}, max_iter=250)\n",
        "lg.fit(X_train1, y_train)\n",
        "# Creating inital data frames to hold scoring metrics.\n",
        "test_scores = pd.DataFrame(columns=['accuracy', 'precision', 'recall', 'f1'])\n",
        "\n",
        "# Predicting target values on testing data\n",
        "pred_target = lg.predict(X_test1)\n",
        "# Inserting the specific metrics for each scoring metric.\n",
        "test_scores.loc[0] = [accuracy_score(y_test, pred_target), precision_score(y_test, pred_target),\n",
        "                          recall_score(y_test, pred_target), f1_score(y_test, pred_target)]\n",
        "# Transposing the test scores so it easier to build dashboard in tableau.\n",
        "test_scores = test_scores.transpose()\n",
        "test_scores = test_scores.reset_index()\n",
        "test_scores.columns = ['metric', 'score']\n",
        "\n",
        "# Creating confusion matrix.\n",
        "conf = confusion_matrix(y_test, pred_target)\n",
        "test_conf = pd.DataFrame(conf, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
        "\n",
        "# Exporting the confusion and scoring metrics csv files.\n",
        "test_scores.to_csv('test_scores_best_model.csv', index=False)\n",
        "test_conf.to_csv('test_conf_best_model.csv', index=True)\n"
      ],
      "metadata": {
        "id": "aZAofYzZPE0l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataframe to hold the features and their corresponding importance.\n",
        "scoring = pd.DataFrame()\n",
        "scoring['feature'] = X_train1.columns\n",
        "scoring['importance'] = lg.coef_[0]\n",
        "scoring = scoring.sort_values(by='importance')\n",
        "\n",
        "scoring.to_csv('feature_importance.csv', index=False)"
      ],
      "metadata": {
        "id": "s24DOkxhgeWi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in joblib\n",
        "import joblib"
      ],
      "metadata": {
        "id": "_CEbZQTcpokS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving my lg model and column_transformer\n",
        "joblib.dump(lg, 'lg.joblib.diab')\n",
        "joblib.dump(column_transformer, 'column_transformer.joblib.diab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ1MtrHxsbgN",
        "outputId": "d6ce6c77-1007-4c9a-8a7e-b667b6ac9c3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['column_transformer.joblib.diab']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}