{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas and numpy library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading in data frame\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "# Deleting unnecessary column and symbolizing all non-diabetic records with 0 instead of 3.\n",
    "del df['Unnamed: 0']\n",
    "df['diabetes_status'] = df['diabetes_status'].replace(3,0)\n",
    "\n",
    "# Isolating target variable. \n",
    "df_target = df['diabetes_status']\n",
    "del df['diabetes_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions to split up and transform our data.\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists to contain the continuous variable names and the discrete variable names.\n",
    "cont = ['age','height_inches','bmi']\n",
    "disc = ['general_health', 'physical_health_days', 'mental_health_days',\n",
    "       'has_health_plan', 'meets_aerobic_guidelines',\n",
    "       'physical_activity_150min', 'muscle_strengthening',\n",
    "       'high_blood_pressure', 'high_cholesterol', 'heart_disease',\n",
    "       'lifetime_asthma', 'arthritis', 'sex', \n",
    "       'education_level', 'income_group', 'smoking_status',\n",
    "       'alcohol_consumption', 'binge_drinking', 'heavy_drinking',\n",
    "       'difficulty_walking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up predictor and target variable into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.30, random_state=22, stratify=df_target)\n",
    "# Utilizing stratify parameters helps ensure that the percentage or diabetic and non-diabetic individuals are around the same in the training and tests sets.\n",
    "\n",
    "# Creating pipeline to logarithmically transform and min max scale all continuous variables.\n",
    "cont_pipeline = Pipeline([\n",
    "    ('log', FunctionTransformer(func=np.log1p)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ])\n",
    "\n",
    "# Creating pipeline to one hot encode all discrete variables while dropping the first.\n",
    "disc_pipeline = Pipeline([\n",
    "    ('one_hot', OneHotEncoder(sparse_output=False, drop='first'))\n",
    "])\n",
    "\n",
    "# Creating a column transform to send all discrete variable to disc_pipeline and all continuous variables to cont_pipeline.\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('cont', cont_pipeline, cont),\n",
    "    ('disc', disc_pipeline, disc),\n",
    "])\n",
    "\n",
    "# Fitting column transform with training data and then transforming training data using the fitted column transformer.\n",
    "X_train1 = column_transformer.fit_transform(X_train)\n",
    "# Transforming testing data using the fitted column transform.\n",
    "X_test1 = column_transformer.transform(X_test)\n",
    "\n",
    "# Creating data frames based on X_train1 and X_test1 for feature importance analysis later one.\n",
    "X_train1 = pd.DataFrame(X_train1)\n",
    "X_test1 = pd.DataFrame(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "The goal of this is to once again explore different models, such as Logistic Regression, Random Forest Classifier, Balanced Random Forest Classifier, Hist Gradient Boosting Classifier, and XGBClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Logistic Regression and f1_score function.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on training data: 0.46\n",
      "f1 score on testing data: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Creating Logistic Regression model with class weight set to balanced and max iterations to 500.\n",
    "lg = LogisticRegression(class_weight='balanced', max_iter=500)        # Max iter was set as the model wasn't able to find a solute with a default of 100 max iterations.\n",
    "\n",
    "# Fitting lg with training data.\n",
    "lg.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating the f1_score on the training data for lg.\n",
    "pred_target = lg.predict(X_train1)\n",
    "f1_train = np.round(f1_score(y_train, pred_target),2)\n",
    "print('f1 score on training data:', f1_train)\n",
    "\n",
    "# Calculating the f1_score on the testing data for lg.\n",
    "pred_target = lg.predict(X_test1)\n",
    "f1_test = np.round(f1_score(y_test, pred_target),2)\n",
    "print('f1 score on testing data:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the 'first_model_building_process.ipynb' file, I noticed that the results—specifically the F1 score on both the training and testing datasets—remained at 0.46. Despite applying different transformations and stratifying the target variable, these changes didn’t significantly impact the model’s performance at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing grid search cv function, stratified fold function for cv, and make scorer function for custom scoring metric\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "{'C': 0.1, 'class_weight': {0: 1, 1: 3}}\n"
     ]
    }
   ],
   "source": [
    "# Creating list to house dictionary of possible parameter values.\n",
    "param_dist = [{\n",
    "    'C': [0.001, .01, .1, .5],\n",
    "    'class_weight': [{0: 1, 1: w} for w in [1, 2, 3, 5.69]]\n",
    "}]\n",
    "\n",
    "#  Specifying initial Logistic Regression model.\n",
    "lg = LogisticRegression(max_iter=500)\n",
    "# Creating GridSearchCV function.\n",
    "grid = GridSearchCV(lg, param_dist, \n",
    "                    cv=StratifiedKFold(n_splits=4),                    # Used to split up the data into 4 sets for cross validation while stratifying the target variable.\n",
    "                    scoring = make_scorer(f1_score, pos_label=1),      # Calling custom scoring function to calculate f1_score on 1 instances.\n",
    "                    verbose=1)                                         # Setting verbose to 1 allows the total number of fits to be printed.\n",
    "# Fitting grid with the training data.\n",
    "grid.fit(X_train1, y_train)\n",
    "\n",
    "# Printing the best parameters from the best estimator from grid.\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the class weight matched the best parameter identified in the logistic regression grid search in 'first_model_building_process.ipynb', the optimal value for the regularization parameter C differed: it was 0.1 in this case, compared to 0.001 in the earlier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on training data: 0.47\n",
      "f1 score on testing data: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Creating Logistic Regression model based on the best estimator from grid.\n",
    "lg = grid.best_estimator_\n",
    "# Fitting lg with training data.\n",
    "lg.fit(X_train1, y_train)\n",
    "# Calculating the f1_score on the training data by lg.\n",
    "pred_target = lg.predict(X_train1)\n",
    "f1_train = np.round(f1_score(y_train, pred_target),2)\n",
    "print('f1 score on training data:', f1_train)\n",
    "# Calculating the f1_score on the testing data by lg.\n",
    "pred_target = lg.predict(X_test1)\n",
    "f1_test = np.round(f1_score(y_test, pred_target),2)\n",
    "print('f1 score on testing data:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training data remains consistent with the grid search best estimator in 'first_model_building_process.ipynb', the F1 score on the testing data improved by .1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RandomForestClassifier function.\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 1.0\n",
      "f1_score on testing data: 0.23\n"
     ]
    }
   ],
   "source": [
    "# Creating a RandomForestClassifier model, rfc, with random state and class_weight set to balanced.\n",
    "rfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Fitting rfc with training data. \n",
    "rfc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = rfc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "\n",
    "# Calculating f1_Score on testing data.\n",
    "pred_target = rfc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is still clear overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing randomized search cv function.\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of the GridSearchCV function is that it exhaustively evaluates all possible combinations of parameters from the specified dictionary. However, this can become computationally expensive when there are many combinations.\n",
    "\n",
    "In contrast, the RandomizedSearchCV function addresses this issue by randomly sampling a pre-specified number of parameter combinations. This approach makes it more efficient, especially when the parameter space is large, while still providing a good chance of finding an optimal or near-optimal set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "{'n_estimators': 30, 'max_features': 'sqrt', 'max_depth': 10, 'class_weight': {0: 1, 1: 3}}\n"
     ]
    }
   ],
   "source": [
    "# Creating list to house dictionary of possible parameter values.\n",
    "param_dist = [{\n",
    "    'n_estimators': [10, 30, 100],\n",
    "    'max_depth': [5, 10],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': [{0:1, 1:w} for w in [1, 3, 5.69]] + [None]\n",
    "}]\n",
    "\n",
    "# Creating initial RandomForestClassifier model.\n",
    "rfc = RandomForestClassifier(random_state=22)\n",
    "\n",
    "# Creating RandomizedSearchCV function.\n",
    "rand_search = RandomizedSearchCV(rfc, param_dist, \n",
    "                                 cv=StratifiedKFold(n_splits=4), \n",
    "                                 scoring = make_scorer(f1_score, pos_label=1), \n",
    "                                 verbose=1, random_state=42)\n",
    "# Fitting rand_search with the training data.\n",
    "rand_search.fit(X_train1, y_train)\n",
    "\n",
    "# Printing the best parameters from the best estimator from rand_search.\n",
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters mentioned above were the same as the best parameters found using RandomizedSearchCV in the 'first model building process.ipynb' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.48\n",
      "f1_score on testing data: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Creating a RandomForestClassifier model, rfc, from the best estimator from rand_search.\n",
    "rfc = rand_search.best_estimator_\n",
    "\n",
    "# Fitting rfc with training data.\n",
    "rfc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = rfc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = rfc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the F1 scores for the training and testing data decreased by 0.03 compared to the best estimator from the RandomizedSearchCV results in 'first model building process.ipynb'. Since the estimator used the same parameters, it appears that the stratification and one-hot encoding strategy did not improve performance as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing BalancedRandomForestClassifier function.\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\conno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\ensemble\\_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\conno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\conno\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "f1_score on training data: 0.6\n",
      "f1_score on testing data: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Creating BalancedRandomForestClassifier model (brfc).\n",
    "brfc = BalancedRandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fitting brfc with training data.\n",
    "brfc.fit(X_train1, y_train)\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = brfc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = brfc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the warnings above are nothing to worry about. From the output, we can observe some overfitting, but the testing f1 score remains decent compared to the initial Random Forest Classifier model testing f1 score (.45 to .23), especially considering that none of the model parameters have been fine-tuned yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "{'n_estimators': 50, 'max_features': None, 'max_depth': 10, 'class_weight': {0: 1, 1: 2}}\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary inside of a list with different parameter possibilities. \n",
    "param_dist = [{\n",
    "    'n_estimators': [5, 25, 50, 100],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'max_features': ['sqrt','log2',None],\n",
    "    'class_weight': [{0:1,1:2} for w in [1, 3, 5.69]] + [None],\n",
    "}]\n",
    "\n",
    "# Creating initial BalancedRandomForestClassifier\n",
    "brfc = BalancedRandomForestClassifier(random_state=42)\n",
    "\n",
    "#Creating RandomizedSearchCV function\n",
    "rand_search = RandomizedSearchCV(rfc, param_dist, \n",
    "                                 cv=StratifiedKFold(n_splits=4), \n",
    "                                 scoring = make_scorer(f1_score, pos_label=1), \n",
    "                                 verbose=1, \n",
    "                                 random_state=42, \n",
    "                                 n_iter=15)                                     # Specifies that 15 different parameter combinations will be independently selected.\n",
    "# Fitting grid with the training data.\n",
    "rand_search.fit(X_train1, y_train)\n",
    "\n",
    "# Printing the best parameters from the best estimator from rand_search.\n",
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.49\n",
      "f1_score on testing data: 0.43\n"
     ]
    }
   ],
   "source": [
    "# Creating a BalancedRandomForestClassifier model, brfc, from the best estimator from rand_search.\n",
    "brfc = rand_search.best_estimator_\n",
    "# Fitting brfc with training data.\n",
    "brfc.fit(X_train1, y_train)\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = brfc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = brfc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overfitting is less apparent than before; however, the F1 score on the testing data has decreased, which is a negative outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing HistGradientBoostingClassifier function.\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.46\n",
      "f1_score on testing data: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Creating a HistGradientBoostingClassifier model, hgbc, with random state set to 42 and class_weight to balanced.\n",
    "hgbc = HistGradientBoostingClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Fitting hgbc with training data.\n",
    "hgbc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = hgbc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = hgbc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the initial HistGradientBoostingClassifier model from the 'first model building process.ipynb' file, the f1 score on the training data is .01 worse while the the f1 score on the testing data is exactly the same with .46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 130 candidates, totalling 520 fits\n",
      "{'min_samples_leaf': 5, 'max_leaf_nodes': 15, 'max_iter': 100, 'max_depth': 20, 'learning_rate': 0.1, 'l2_regularization': 0.5, 'class_weight': {0: 1, 1: 3}}\n"
     ]
    }
   ],
   "source": [
    "# Creating list to house dictionary of possible parameter values.\n",
    "param_dist = [{\n",
    "    'learning_rate': [.1, .5, .9],\n",
    "    'max_iter': [10, 25, 50, 100],\n",
    "    'max_leaf_nodes': [5, 15],\n",
    "    'max_depth': [5,10,20],\n",
    "    'min_samples_leaf': [5,10],\n",
    "    'l2_regularization': [.1, .25, .5, 1],\n",
    "    'class_weight': [{0:1, 1:w} for w in [1, 2, 3, 5.69]]\n",
    "}]\n",
    "\n",
    "# Creating initial HistGradientBoostingClassifier model.\n",
    "hgbc = HistGradientBoostingClassifier(random_state=42)\n",
    "# Creating initial RandomizedSearchCV function.\n",
    "rand_search = RandomizedSearchCV(hgbc, param_distributions=param_dist, \n",
    "                                scoring= make_scorer(f1_score, pos_label=1),\n",
    "                                cv=StratifiedKFold(n_splits=4), \n",
    "                                verbose=1, n_iter=130, random_state=22)\n",
    "# Fitting rand_search with training data.\n",
    "rand_search.fit(X_train1, y_train)\n",
    "\n",
    "# Printing the best parameters from the best estimator from rand_search.\n",
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.48\n",
      "f1_score on testing data: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Creating a HistGradientBoostingClassifier model, hgbc, based on best estimator from rand_search.\n",
    "hgbc = rand_search.best_estimator_\n",
    "\n",
    "# Fitting hgbc with training data.\n",
    "hgbc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = hgbc.predict(X_train1)\n",
    "f1_train = f1_score(y_train, pred_target, pos_label=1)\n",
    "print('f1_score on training data:', np.round(f1_train,2))\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = hgbc.predict(X_test1)\n",
    "f1_test = f1_score(y_test, pred_target, pos_label=1)\n",
    "print('f1_score on testing data:', np.round(f1_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and testing scores did both increase, but the testing score is still subpar at .48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing XGBClassifier function.\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.5\n",
      "f1_score on testing data: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Creating a XGBClassifier model (xgbc) with random_state set to 42 and scale_pos_weight set to 5.69.\n",
    "xgbc = XGBClassifier(random_state=42, scale_pos_weight=5.69)\n",
    "\n",
    "# Fitting xgbc with training data. \n",
    "xgbc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = xgbc.predict(X_train1)\n",
    "f1_train = np.round(f1_score(y_train, pred_target),2)\n",
    "print('f1_score on training data:', f1_train)\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = xgbc.predict(X_test1)\n",
    "f1_test = np.round(f1_score(y_test, pred_target),2)\n",
    "print('f1_score on testing data:', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n",
      "{'scale_pos_weight': 3, 'max_depth': 2, 'gamma': 0.1, 'eta': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Creating list to house dictionary with possible parameter values.\n",
    "param_dist = [{\n",
    "    'eta': [0.1, 0.3, 0.7, 0.9],\n",
    "    'gamma': [0.1, 0.4, 1],\n",
    "    'max_depth': [2, 6, 9, 14],\n",
    "    'scale_pos_weight': [1, 2, 3, 5.69]\n",
    "}]\n",
    "# Creating initial XGBClassifier model.\n",
    "xgbc = XGBClassifier(random_state=42)\n",
    "# Creating RandomizedSearchCV function.\n",
    "rand_search = RandomizedSearchCV(xgbc, param_distributions=param_dist, \n",
    "                                scoring=make_scorer(f1_score, pos_label=1), \n",
    "                                cv=StratifiedKFold(n_splits=4), \n",
    "                                verbose=1, n_iter=20, random_state=22)\n",
    "# Fitting rand_search with training data.\n",
    "rand_search.fit(X_train1, y_train)\n",
    "\n",
    "# Printing the best parameters from the best estimator from rand_search.\n",
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.48\n",
      "f1_score on testing data: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Creating a XGBClassifier model, xgbc, based on the best estimator from rand_search.\n",
    "xgbc = rand_search.best_estimator_\n",
    "\n",
    "# Fitting xgbc with training data. \n",
    "xgbc.fit(X_train1, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = xgbc.predict(X_train1)\n",
    "f1_train = np.round(f1_score(y_train, pred_target),2)\n",
    "print('f1_score on training data:', f1_train)\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = xgbc.predict(X_test1)\n",
    "f1_test = np.round(f1_score(y_test, pred_target),2)\n",
    "print('f1_score on testing data:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This f1 score on the testing data is the same as the f1 score on the testing data on the 'best' HistGradientBoostingClassifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating data frame with all feature names and corresponding importance.\n",
    "feature_importance = pd.DataFrame()\n",
    "feature_importance['features'] = xgbc.feature_names_in_\n",
    "feature_importance['importance'] = xgbc.feature_importances_\n",
    "\n",
    "# Printing shape of data frame that only contains feature with no importance.\n",
    "print(feature_importance[feature_importance['importance'] == 0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 50 variables that have no importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new data frame with features that have some importance.\n",
    "impt_features = feature_importance.loc[feature_importance['importance'] != 0]\n",
    "# Creating a list with the values from the 'features' columns.\n",
    "imp_cols = impt_features['features'].values.astype(int).tolist()\n",
    "\n",
    "# Creating new training and testing predictor data frame with the important variables.\n",
    "X_train1_imp = X_train1[imp_cols]\n",
    "X_test1_imp = X_test1[imp_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training data: 0.48\n",
      "f1_score on testing data: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Creating XGBClassifier,xgbc, based on the best parameters from the rand_search best estimator.\n",
    "xgbc = XGBClassifier(scale_pos_weight= 3, max_depth= 2, gamma= 0.1, eta= 0.3, random_state=42)\n",
    "\n",
    "# Fitting xgbc with training data. \n",
    "xgbc.fit(X_train1_imp, y_train)\n",
    "\n",
    "# Calculating f1_score on training data.\n",
    "pred_target = xgbc.predict(X_train1_imp)\n",
    "f1_train = np.round(f1_score(y_train, pred_target),2)\n",
    "print('f1_score on training data:', f1_train)\n",
    "\n",
    "# Calculating f1_score on testing data.\n",
    "pred_target = xgbc.predict(X_test1_imp)\n",
    "f1_test = np.round(f1_score(y_test, pred_target),2)\n",
    "print('f1_score on testing data:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I hoped that removing non-important variables would improve the F1 score on the testing data, it remained unchanged at 0.48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial goal of this notebook was to determine whether one-hot encoding discrete variables would enable the model to make more accurate assumptions about each discrete variable. Additionally, stratifying the target variable when splitting the data into training and testing sets aimed to help the model generalize better to unseen data. However, while these adjustments did not decrease the F1 score for the target variable, they also did not result in any significant improvement.\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "Moving forward, the focus will shift to evaluating not only the F1 score for diabetic instances but also metrics such as recall, precision, and the confusion matrix. By analyzing these metrics, we can hopefully identify the factors affecting the F1 score. This analysis will primarily be conducted using HistGradientBoostClassifier and XGBClassifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
