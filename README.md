# Health Data Insights Project

## Project Overview
This project utilizes machine learning techniques to predict diabetes status based on health and behavioral data from the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) survey. By analyzing factors such as physical activity and BMI, the model aims to identify individuals at risk for diabetes, providing insights that could aid in early detection and intervention.

## Completed Work
* **[Initial Notebook](https://github.com/Cstan1987stat/-Health-Data-Insights-Predicting-Diabetes-with-Machine-Learning/blob/main/notebooks/inital%20notebook.ipynb)** : This notebook outlines how the dataset was obtained, explains the features and their values, and details the process of selecting relevant variables. It also includes filtering and transforming feature values, renaming columns, and conducting basic exploratory data analysis on both continuous and discrete features.
* **[First Model Buidling Process](https://github.com/Cstan1987stat/-Health-Data-Insights-Predicting-Diabetes-with-Machine-Learning/blob/main/notebooks/first%20model%20building%20process.ipynb)** Applied preprocessing by log-transforming and standard scaling continuous features, and standard scaling discrete features. Trained Logistic Regression, Random Forest, and Histogram-based Gradient Boosting Classifier models. Improved the Logistic Regression model’s F1-score from 0.28 to 0.46 by adjusting the class_weight parameter. Used RandomizedSearchCV to tune hyperparameters and achieved an F1-score of 0.48 with the Histogram-based Gradient Boosting Classifier on both training and testing sets.
* **[A Second Attempt](https://github.com/Cstan1987stat/-Health-Data-Insights-Predicting-Diabetes-with-Machine-Learning/blob/main/notebooks/a%20second%20attempt.ipynb)** : Changed the preprocessing pipeline by log-transforming and scaling continuous features, and one-hot encoding discrete features. Tuned Logistic Regression, Random Forest, Histogram-based Gradient Boosting, and XGBoost models. Despite efforts, the highest F1-score achieved remained at 0.48.
* **[Analyzing the Scoring](https://github.com/Cstan1987stat/-Health-Data-Insights-Predicting-Diabetes-with-Machine-Learning/blob/main/notebooks/analyzing%20the%20scoring.ipynb)** : Evaluated model performance using confusion matrices for both Histogram-based Gradient Boosting and XGBoost models. Experimented with adjusting probability thresholds for classification and applied SMOTE to address class imbalance. Neither threshold tuning nor SMOTE led to significant improvements; SMOTE, in particular, caused the model to overfit and perform poorly on test data.
* **[A Fourth Attempt](https://github.com/Cstan1987stat/-Health-Data-Insights-Predicting-Diabetes-with-Machine-Learning/blob/main/notebooks/a_fourth_attempt.ipynb)** : Revised the preprocessing strategy by splitting discrete features into ordinal and nominal categories. Tuned a Logistic Regression model and evaluated performance on the test set. Successfully exported the final trained model and the corresponding column transformer using joblib, which were later deployed in a **[Streamlit App](https://diabetic-prediction-app-4321.streamlit.app/)**.

The page for the code to create the streamlit app can be found here: **[GitHub Streamlit App Page](https://github.com/Cstan1987stat/Diabetic-Prediction-App-Site)**.

## Conclusion
This project was a valuable learning experience in both machine learning and the practical realities of working with large datasets. Most of the development took place in Visual Studio Code, and I worked with the full dataset of around 200,000 rows. This placed a heavy load on my laptop’s processing power and memory, often causing it to overheat. I believe this stress contributed to my old computer eventually breaking down—something I hadn’t anticipated when starting the project. This experience taught me the importance of understanding the resource demands of data-intensive tasks and being mindful of the tools and hardware needed to support them.

In terms of technical lessons, two main areas stand out. First, I would invest more time in a thorough and complex data analysis phase before modeling. Second, I would focus more on the model’s output probabilities rather than just the predicted class labels, especially in imbalanced classification problems like this one. These insights will guide how I approach similar projects in the future.
